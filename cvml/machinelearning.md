# 1. Machine Learning


# 2. 降维

* 对原始数据提取的特征进行降维
  * 减少冗余和噪声
  * 提高特征的表达能力
  * 降低训练复杂度

## 2.1. PCA Principal Components Analysis

* 如何定义主成分
  * 三位空间中的点分布在同一个平面上, 用三维坐标会有冗余
  * 找出平面, 然后用该平面上的二维坐标表示点, 即完成了数据降维
* 主成分分析
  * 特点: 线性, 非监督, 全局
  * 目标: 最大化投影方差, 让数据在主轴上投影的方差最大
  * 加入核映射, 得到核主成分分析(KPCA)
  * 加入流形映射的降维方法, eg. 等距映射, 局部线性嵌入, 拉普拉斯特征映射
  
### 2.1.1. 最大方差理论

* 根据信息理论, 信号具有较大方差, 噪声具有较小方差, 信号与噪声的比例成为信噪比
* PCA 的目标即最大化投影方差, 让数据在主轴上的投影的方差最大

推导过程:
1. 给定一组数据向量 $\{v_1,v_2,...,v_n \}$, 先将其中心化得到 $\{x_1,x_2,...,x_n \}$  
2. 向量内积表示为第一个向量投影到第二个向量上的长度, 对于单位向量 $\omega$, 投影可以表示为 $(x_i,\omega)=x_i^T\omega$  
3. 找到一个投影方向$\omega$, 使得$\{x_1,x_2,...,x_n \}$ 在该方向上的投影方差尽可能大. 且易知中心化后的向量投影后的均值也为0

- 投影后的方差为: $D(x)=\frac{1}{n}\sum_{i=1}^n(x_i^T\omega)^2=\omega^T(\frac{1}{n}\sum_{i=1}^nx_ix_i^T)\omega$  

4. 可以发现$(\frac{1}{n}\sum_{i=1}^nx_ix_i^T)$ 就是样本协方差矩阵
5. 将其化简为Σ后, 可以将PCA目标转化为 $max(\omega^T\sum\omega) s.t. (\omega^t\omega=1)$
6. 引入拉格朗日乘子, 求导后可推出 $\sum\omega=\lambda\omega\quad\rightarrow D(x)=\lambda$
- 即投影后的方差就是协方差矩阵的特征值, 目标的最大方差也就是最大特征值, 而目标最佳投影方向也就是最大特征值的特征向量, 

求解方法:
1. 对样本数据进行中心化处理
2. 求样本的协方差矩阵
3. 特征值分解, 并按大小排列
4. 降维目标是d维的话, 就选择前 d个特征值的特征向量, 然后进行映射, 方差较小的特征(噪声)自动被舍弃  
- $x_i'^T=[\omega_1^Tx_1,\omega_2^Tx_2,...,\omega_d^Tx_d]$
- 容易得知降维后得信息占比: 



### 2.1.2. 最小平方误差理论

