


# Hidden Markov Model HMM

隐马尔可夫模型, 是一个在当前世界应用极其广泛的数学模型  

一个 HMM 模型有三个要素, 可以表示成 $\lambda = (A,B,\Pi)$
* A : 状态转移概率矩阵, 假设共有 N 种不可见的状态, 状态间的互相转移的概率为 $A=[a_{ij}]_{N\times N}$
* B : 观测概率矩阵, N 种不可见的状态共计可以有 M 种观测, 则处在 j 状态下得到观测 k 的概率为 $B=[b_j(k)]_{N\times M}$, 又称发射概率
* $\Pi$ : 初始状态概率向量, 即初始时间时每个状态的概率

一个 HMM 的完整五元组, 还要再加上
* Q : 所有可能的状态集合
* V : 所有可能的观测集合

HMM 是一个关于 `时序` 的 `概率模型`, 一个不可观测的状态随机序列 S (state sequence) 是一个隐藏的马尔科夫链, 每个状态随机生成一个观测, 由此产生的观测的随机序列 O (observation sequence).

* 观测序列 $O=O_1,O_2,...,O_T$
* 状态序列 $S=S_1,S_2,...,S_T$

三个基本问题:
1. 观测问题 : 给定模型 $\lambda = (A,B,\Pi)$ , 求一个观测序列 O 出现的概率
2. 学习问题 : 给定观测序列 O, 估计模型的参数, 使得该观测序列 O 的概率最大
3. 预测问题(解码问题) (词性标注问题): 给定模型和观测序列O, 求观测序列 O 所对应的概率最大的状态序列 I 
* 目前 概率计算问题和解码问题都有最优解, 而学习问题则是最复杂的问题, 需要解决预测子问题 

## 前向后向算法 解决 观测问题

因为序列和模型概率都全部已知, 理论上可以通过暴力求解对应概率, 而实际上由于隐状态的个数一般非常大, 计算全部路径的计算量是 N^T , 不可能应用在实际中


流行解法: 前向后向算法 (基于动态规划)

前向算法:
* 定义二维 DP 表ti , 记录到沿着给定观测 O 的 t 时刻时状态为 i 的概率
* 初值: `DP[1][i]` = $\pi_i b_i(o_1)$  
* `DP[t+1][i]` = $(\sum^N_{j=1}DP_{[t][j]}a_{ji}) b_i(o_{t+1})$
* 求结果 $P(O|\lambda)=\sum_{i=1}^NDP_{[T][i]}$
* 复杂度 O(TN^2)

后向算法: 仅仅只是更改了计算方向
* 初值: `DP[T][i]=1`
* `DP[t][i]`=$\sum^N_{j=1}DP_{[t+1][j]}a_{ij}b_j(o_{t+1})$
* 求结果  $P(O|\lambda)=\sum_{i=1}^NDP_{[T][i]}\Pi_ibi(o_1)$
* 感觉从计算上来说多了一个乘法, 发射概率需要在 sum 里的每个单位都乘进去

## 维比特算法 

维特比算法之所以重要, 是因为凡是使用隐含马尔可夫模型描述的问题都可以用它来解码: 
* 即, 该算法是应用于解码问题的一个算法
* 词性标注问题的定义:
  * 有一句已经分词好的句子 n 个单词, 有定义好的词性字典 m 种词性
  * 求每一个单词的词性
  * 共有n^m 种可能
* 算法构成:
  * 输入 : 模型 $\lambda=(A,B,\Pi)$ 观测$O=(o_1, o_2,...,o_T)$
  * 输出 : 最优(解码/词性/状态)序列 $I=(i_1,i_2,...,i_T)$

算法流程
* 初值: 
  * `DP[1][i]` = $\pi_i b_i(o_1)$  同前向算法一样 , t=1时 各个状态 满足 o1 的概率
  * `DP2[1][i]=0`  置全0
* 递推:
  * `DP[t][i]` = $max_{j\in[1,N]}(DP[t-1][j]a_{ji}) b_i(o_t)$, 解释: t-1时刻的dp里转移到状态 i 的最大概率, 然后发射出 ot 的概率
  * `DP2[t][i]` = $argmax_{j\in[1,N]}(DP[t-1][j]a_{ji})$ , 解释: 仅仅只是记录对应转移到 $i_t$ 的最大概率的 $i_{t-1}$
* 中止:
  * $i_T=argmax_{j\in[1,N]}(DP[T][j])$ 最终时刻里概率最大的状态即为最优解里的 $i_T$
* 回溯:
  * $i_{t-1}=DP2[t][i_t]$, 一个 O(n) 的查表而已, 得到最终的完整 $I$ 序列