# 数学基础

字母加上小斜杠的字符一般被用来表示数域  

* $\mathbb{Z}$ 整数域
* $\mathbb{Q}$ 有理数
* $\mathbb{R}$ 实数域
* $\mathbb{C}$ 复数域


## 复数域

复数域补足了实数域的不完备的地方
* 负数可以开根
* 实数域需要引入特殊的无穷 $\infty$ 数字, 从数学上来说不完备, 复数域上无穷则是一个普通的数


复变函数的定义:
* $z=x+iy$ 称 x 是 real, y 是 imaginary 虚部
* $f(z)=u+iv=u(x,y)+iv(x,y)\in \mathbb{C}$
  * $x,y,u,v \in \mathbb{R}$

复变函数的导数:
* 标准导数定义:
$$f'(z_o)=\lim_{z\rArr z_0}\frac{f(z)-f(z_0)}{z-z_0}$$
* 若复变函数 $f(z)=u+iv$ 可导, 则 u,v 存在牵连关系
  * 至少沿着 x,y (实轴和虚轴)的方向趋近得到的表达式相同
  * 沿着实轴: 
    * $z-z_0=\triangle x$
    * $f'(z_o)=\lim_{\triangle x \rArr 0}\frac{\triangle u + i\triangle v}{\triangle x}==\frac{\partial u}{\partial x}+i\frac{\partial v}{\partial x}$
  * 沿着虚轴, 同理:
    * $z-z_0=i\triangle y$
    * $f'(z_0)=\lim_{\triangle x \rArr 0}\frac{\triangle u + i\triangle v}{i\triangle y}=-i\frac{\partial u}{\partial y}+\frac{\partial v}{\partial y}$
* 复变函数保留全套求导公式

# 级数



## 正交变换

正交变换: 是信号变化的一系列统称
* 傅立叶变换
* 离散余弦变换
* 小波变换
* 多尺度几何分析（超小波）

### Fourier Transform  

### DCT 离散余弦变换  Discrete Cosine Transform

是傅里叶变换的一个变种, 类似于离散傅里叶变换, 但是去除了虚数部分  
* 相当于一个长度是它的两倍的对实偶函数进行的离散傅里叶变换  
* DCT 有8种标准类型
* 有两种相关变换  离散正弦变换, 改进的离散余弦变换
* 用途: 对数字信号 (信号, 图像) 进行有损压缩, e.g. JPEG

形式化定义: 线性的可逆函数 $F: R^n \rarr R^n$, 把 n 个实数变换到另外 n 个实数的操作  


#### DCT-II 

$$f_m=\sum^{n-1}_{k=0}x_kcos[\frac{\pi}{n}m(k+\frac{1}{2})]$$

最常用的一种 DCT, 通常被直接称作 DCT    

#### DCT-III  

DCT-II 的逆变换, 通常称为 逆离散余弦变换 


### Wavelet Transform 小波变换




# 概率论



## 分布


### 泊松分布 poisson distribution

是一种统计与概率学里常见到的离散机率分布 (discrete probability distribution)

$$P(X=k)=\frac{\lambda^k}{k!}, k=0,1,...$$

泊松分布的特征
* 方差和数学期望均为 $\lambda$
* $\lambda$ 是单位内随机事件的平局发生次数
* 泊松分布适合描述单位时间内随机事件的发生次数


# Hidden Markov Model HMM

隐马尔可夫模型, 是一个在当前世界应用极其广泛的数学模型  

一个 HMM 模型有三个要素, 可以表示成 $\lambda = (A,B,\Pi)$
* A : 状态转移概率矩阵, 假设共有 N 种不可见的状态, 状态间的互相转移的概率为 $A=[a_{ij}]_{N\times N}$
* B : 观测概率矩阵, N 种不可见的状态共计可以有 M 种观测, 则处在 j 状态下得到观测 k 的概率为 $B=[b_j(k)]_{N\times M}$, 又称发射概率
* $\Pi$ : 初始状态概率向量, 即初始时间时每个状态的概率

一个 HMM 的完整五元组, 还要再加上
* Q : 所有可能的状态集合
* V : 所有可能的观测集合

HMM 是一个关于 `时序` 的 `概率模型`, 一个不可观测的状态随机序列 S (state sequence) 是一个隐藏的马尔科夫链, 每个状态随机生成一个观测, 由此产生的观测的随机序列 O (observation sequence).

* 观测序列 $O=O_1,O_2,...,O_T$
* 状态序列 $S=S_1,S_2,...,S_T$

三个基本问题:
1. 观测问题 : 给定模型 $\lambda = (A,B,\Pi)$ , 求一个观测序列 O 出现的概率
2. 学习问题 : 给定观测序列 O, 估计模型的参数, 使得该观测序列 O 的概率最大
3. 预测问题(解码问题) (词性标注问题): 给定模型和观测序列O, 求观测序列 O 所对应的概率最大的状态序列 I 
* 目前 概率计算问题和解码问题都有最优解, 而学习问题则是最复杂的问题, 需要解决预测子问题 

## 前向后向算法 解决 观测问题

因为序列和模型概率都全部已知, 理论上可以通过暴力求解对应概率, 而实际上由于隐状态的个数一般非常大, 计算全部路径的计算量是 N^T , 不可能应用在实际中


流行解法: 前向后向算法 (基于动态规划)

前向算法:
* 定义二维 DP 表ti , 记录到沿着给定观测 O 的 t 时刻时状态为 i 的概率
* 初值: `DP[1][i]` = $\pi_i b_i(o_1)$  
* `DP[t+1][i]` = $(\sum^N_{j=1}DP_{[t][j]}a_{ji}) b_i(o_{t+1})$
* 求结果 $P(O|\lambda)=\sum_{i=1}^NDP_{[T][i]}$
* 复杂度 O(TN^2)

后向算法: 仅仅只是更改了计算方向
* 初值: `DP[T][i]=1`
* `DP[t][i]`=$\sum^N_{j=1}DP_{[t+1][j]}a_{ij}b_j(o_{t+1})$
* 求结果  $P(O|\lambda)=\sum_{i=1}^NDP_{[T][i]}\Pi_ibi(o_1)$
* 感觉从计算上来说多了一个乘法, 发射概率需要在 sum 里的每个单位都乘进去

## 维比特算法 

维特比算法之所以重要, 是因为凡是使用隐含马尔可夫模型描述的问题都可以用它来解码: 
* 即, 该算法是应用于解码问题的一个算法
* 词性标注问题的定义:
  * 有一句已经分词好的句子 n 个单词, 有定义好的词性字典 m 种词性
  * 求每一个单词的词性
  * 共有n^m 种可能
* 算法构成:
  * 输入 : 模型 $\lambda=(A,B,\Pi)$ 观测$O=(o_1, o_2,...,o_T)$
  * 输出 : 最优(解码/词性/状态)序列 $I=(i_1,i_2,...,i_T)$

算法流程
* 初值: 
  * `DP[1][i]` = $\pi_i b_i(o_1)$  同前向算法一样 , t=1时 各个状态 满足 o1 的概率
  * `DP2[1][i]=0`  置全0
* 递推:
  * `DP[t][i]` = $max_{j\in[1,N]}(DP[t-1][j]a_{ji}) b_i(o_t)$, 解释: t-1时刻的dp里转移到状态 i 的最大概率, 然后发射出 ot 的概率
  * `DP2[t][i]` = $argmax_{j\in[1,N]}(DP[t-1][j]a_{ji})$ , 解释: 仅仅只是记录对应转移到 $i_t$ 的最大概率的 $i_{t-1}$
* 中止:
  * $i_T=argmax_{j\in[1,N]}(DP[T][j])$ 最终时刻里概率最大的状态即为最优解里的 $i_T$
* 回溯:
  * $i_{t-1}=DP2[t][i_t]$, 一个 O(n) 的查表而已, 得到最终的完整 $I$ 序列