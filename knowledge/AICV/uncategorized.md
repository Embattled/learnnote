


## Lipschitz 常数

单层神经网络的 Lipschitz 常数 为该层网络权重的谱范数(最大奇异值)


用于优化神经网络, 使得其 每层网络的 Lipschitz 常数归一化为 1, 称为谱归一化

解决 GAN 网络中梯度的消失或者爆炸的模式


## Variational Autoencoder, VAE 变分自编码器


传统 自编码器 的任务目标是 直接重构数据, VAE 的关键改进是 对潜在变量 z 进行概率建模, 并使得 z 满足某种分布 (通常为标准正态分布)


VAE 的特点, 与传统 自动编码器区分的点 是: VAE 的潜在空间在设计的时候是连续的
* 因此可以轻松的采样和插值, 这对于 生成任务来说非常有用
* 编码器的输出不再是一个维度为 n 的特征向量, 而是两个, 分别作为分布的 均值和标准差

三个关键模块
* 编码器 : 输入数据 x 提取特征, 输出潜在空间中的 概率分布参数: 均值和方差
* 潜在变量: 引入一组潜在变量 z, 潜在变量源自于一个简单的先验分布 (标准正态), 模型学习到的后验近似 $q(z|x)$ 必须同此先验分布保持一致, 作为稳定生成过程和良好特征表达能力的证明
* 解码器: 从潜在变量 映射回元数据空间, 生成与输入数据分布近似的新样本. 解码器通常是与编码器对称的神经网络. 用于参数化生成分布 $p_\theta(x|z)$  


VAE 流程
* 编码: 输入数据 x, 获得潜在分布 $\mu, \sigma$
* 潜在空间采样 $z=\mu + \sigma N(0,1)$
* 解码: 输入潜在变量 z 输出 $p(x+z)$, 即重建数据的分布
  * 这一步计算称为 重参数化技巧, 使得 $N(\mu, \sigma)$ 这一分布函数可导
* 计算损失, 反向传播

信息损失的直接表示
$$
KL(P||Q) = \int P(x)\log\frac{P(x)}{Q(x)}dx \\
KL(P||Q) = \sum_x P(x)\log\frac{P(x)}{Q(x)} =  \sum_x P(x)(\log P(x)-\log Q(x))
$$

* 分布 P 的信息熵 $\sum_x P(x)\log P(x)$ 
* Q 下对 P 的交叉熵 $\sum_x P(x)\log Q(x)$ 
* KL 损失就是用 Q 来表示 P 的时候所带来的 信息损失, KL 散度越大说明 P 和 Q 的差异越大, 除此之外 KL 的性质有
  * 非负性 
  * 非对称性 $D_{KL}(P//Q) \neq D_{KL}(Q//P)$
  * 可能是因为 非对称性 导致 KL 不能直接看作距离, 但是仍然可以作为优化目标

VAE 通过最大化 证据下界 (Evidence Lower Bound, ELBO) 来优化模型, 损失函数分为两部分:
1. 重构损失 Reconstruction Loss
   1. 衡量解码器重构数据的能力, 通过 **交叉熵** 或者 均方误差 来优化
2. KL 散度 (Kullback-Leibler Divergence)
   1. 约束模型输出的近似分布 q(z|x) 使得其尽可能接近先验分布 p(z), 通常先验分布为标准正态分布
   2. 在 概率论和信息论中, KL 用来表示 Q 分布(近似分布) 和 目标分布P 之间的 **相对熵** (信息损失)

$$
优化目标: \underset{\theta}{max}\sum_i \log p_\theta(x^{(i)})=\sum_i \log \int p_\theta(x^{(i)}, z)dz \\
ELBO(\theta, \phi;x) = E_{q_\phi(z|x)[\log p_\theta(x|z)]} - D_{KL}(q_\phi(z|x)||p_\theta(z)) \\
Loss = -ELBO(\theta, \phi;x)
$$


 